# Arsitektur Sistem - Distributed Synchronization System

## ğŸ“ Overview

Sistem ini mengimplementasikan distributed synchronization menggunakan Raft Consensus Algorithm sebagai fondasi untuk koordinasi antar node. Sistem terdiri dari beberapa komponen utama yang bekerja bersama untuk menyediakan distributed locking, queueing, dan caching.

## ğŸ—ï¸ Komponen Utama

### 1. Raft Consensus Layer

**Tujuan**: Menyediakan consensus dan koordinasi antar nodes

**Komponen**:

- **Leader Election**: Algoritma untuk memilih leader node
- **Log Replication**: Replikasi state changes ke semua nodes
- **Safety Guarantee**: Memastikan consistency across cluster

**States**:

- **Follower**: State default, menerima updates dari leader
- **Candidate**: State transisi saat election
- **Leader**: Node yang mengkoordinasi cluster

**Flow Leader Election**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Follower â”‚â”€â”€â”€timeoutâ”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     lostâ”€â”€â”€â”€â”€â”€â”€â”€â”‚ Candidate â”‚
     election    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                    won election
                         â”‚
                         â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Leader  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Distributed Lock Manager (DLM)

**Tujuan**: Koordinasi akses ke shared resources

**Fitur**:

- **Shared Locks**: Multiple readers concurrent access
- **Exclusive Locks**: Single writer access
- **Deadlock Detection**: Cycle detection dalam wait-for graph
- **Lock Timeout**: Automatic lock release

**Lock Acquisition Flow**:

```
Client Request
     â”‚
     â–¼
Is Leader?â”€â”€Noâ”€â”€â–ºForward to Leader
     â”‚
    Yes
     â”‚
     â–¼
Check Lock Availability
     â”‚
     â”œâ”€â”€Availableâ”€â”€â–ºGrant Lockâ”€â”€â–ºReplicate via Raft
     â”‚
     â””â”€â”€Not Availableâ”€â”€â–ºAdd to Wait Queue
```

**Deadlock Detection**:

- Build wait-for graph
- Detect cycles menggunakan DFS
- Resolution: Abort salah satu transaction (future work)

### 3. Distributed Queue System

**Tujuan**: Message passing dengan reliability guarantee

**Fitur**:

- **Consistent Hashing**: Distribute messages across nodes
- **At-Least-Once Delivery**: Message tidak hilang
- **Message Persistence**: Saved to Redis
- **Consumer Groups**: Multiple consumers

**Message Flow**:

```
Producer
   â”‚
   â–¼
Enqueueâ”€â”€â–ºConsistent Hashâ”€â”€â–ºTarget Node
   â”‚
   â–¼
Redis Persistence
   â”‚
   â–¼
Consumer Dequeue
   â”‚
   â”œâ”€â”€Successâ”€â”€â–ºACKâ”€â”€â–ºRemove from Queue
   â”‚
   â””â”€â”€Failureâ”€â”€â–ºNACKâ”€â”€â–ºRequeue
```

**Consistent Hashing**:

- Virtual nodes untuk load balancing
- Minimal disruption saat node failure
- Ring-based key distribution

### 4. Distributed Cache (MESI Protocol)

**Tujuan**: Fast data access dengan consistency guarantee

**MESI States**:

- **Modified (M)**: Cache dirty, exclusive ownership
- **Exclusive (E)**: Cache clean, exclusive ownership
- **Shared (S)**: Cache clean, dapat ada di multiple caches
- **Invalid (I)**: Cache tidak valid

**State Transitions**:

```
        Read Hit
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      â–¼      â”‚
[I]â”€â”€â”€Readâ”€â”€â–º[S]â”€â”€â”´â”€â”€â–º[E]
 â”‚            â”‚         â”‚
Write        Write    Write
 â”‚            â”‚         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º[M]â—„â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Cache Coherence Flow**:

```
Node 1 Write Request
        â”‚
        â–¼
    Invalidate Other Caches
        â”‚
        â”œâ”€â”€â–ºNode 2: Mark [I]
        â”œâ”€â”€â–ºNode 3: Mark [I]
        â””â”€â”€â–ºNode 4: Mark [I]
        â”‚
        â–¼
    Update Local Cache [M]
```

## ğŸ”„ Interaksi Antar Komponen

### Complete System Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Client Layer                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               FastAPI HTTP Server                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚Lock API  â”‚  â”‚Queue API â”‚  â”‚Cache API â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Application Components Layer                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   DLM    â”‚  â”‚  Queue   â”‚  â”‚  Cache   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Raft Consensus Layer                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Leader    â”‚  â”‚  Follower 1  â”‚  â”‚  Follower 2  â”‚ â”‚
â”‚  â”‚  Election   â”‚  â”‚              â”‚  â”‚              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                 â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Communication Layer                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   Message    â”‚  â”‚    Failure    â”‚                  â”‚
â”‚  â”‚   Passing    â”‚  â”‚   Detection   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Network Layer                           â”‚
â”‚              HTTP/JSON Messages                         â”‚
â”‚         (Could be gRPC for production)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Persistence Layer (Redis)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Raft    â”‚  â”‚  Queue   â”‚  â”‚  Cache   â”‚            â”‚
â”‚  â”‚  State   â”‚  â”‚Messages  â”‚  â”‚  Backup  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ›¡ï¸ Fault Tolerance

### Network Partition Handling

**Scenario 1: Leader Isolation**

```
Normal:                 Partition:
â”Œâ”€Lâ”€â”                   â”Œâ”€Lâ”€â”  â”‚  â”Œâ”€Fâ”€â”
â”‚   â”‚â—„â”€â”€â–ºâ”Œâ”€Fâ”€â”         â”‚(isolated) â”‚   â”‚â—„â”€â”€â–ºâ”Œâ”€Fâ”€â”
â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜         â””â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜
  â–²        â–²                   â”‚    â”‚        â”‚
  â”‚        â”‚                   â”‚    â”‚   elect new
  â–¼        â–¼                   â”‚    â–¼      leader
â”Œâ”€Fâ”€â”    â”Œâ”€Fâ”€â”               â”‚  â”Œâ”€Fâ”€â”    â”Œâ”€L'â”€â”
â””â”€â”€â”€â”˜    â””â”€â”€â”€â”˜               â”‚  â””â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”˜

Result: Majority partition elects new leader
```

**Scenario 2: Split Brain Prevention**

- Raft requires majority untuk decisions
- Minority partition tidak bisa commit changes
- Prevents data inconsistency

### Node Failure Recovery

**Detection**:

1. Heartbeat timeout
2. Failed RPC calls
3. Failure detector notification

**Recovery Steps**:

```
1. Detect Failure
   â”‚
   â–¼
2. Update Cluster Membership
   â”‚
   â–¼
3. Redistribute Load
   â”‚  â”Œâ”€â”€â–º Queue Messages
   â”‚  â”œâ”€â”€â–º Cache Entries
   â”‚  â””â”€â”€â–º Lock Ownership
   â”‚
   â–¼
4. Start Recovery Protocol
   â”‚
   â–¼
5. Node Rejoin
   â”‚
   â–¼
6. State Synchronization
```

## ğŸ“Š Performance Considerations

### Scalability

**Horizontal Scaling**:

- Add more nodes untuk increase capacity
- Consistent hashing minimize data movement
- Lock distribution via Raft

**Bottlenecks**:

- Leader sebagai single point for writes
- Network latency untuk consensus
- Redis I/O untuk persistence

**Optimizations**:

1. **Batching**: Group operations
2. **Pipelining**: Async message passing
3. **Caching**: Local cache untuk reads
4. **Connection Pooling**: Reuse connections

### Latency Analysis

**Lock Acquisition**:

- Local: 1-2 ms
- Leader: 5-10 ms (1 RTT + consensus)
- Follower: 10-20 ms (2 RTT + consensus)

**Queue Operations**:

- Enqueue: 2-5 ms (with persistence)
- Dequeue: 1-3 ms (from memory)

**Cache Operations**:

- Hit: 0.5-1 ms
- Miss: 5-10 ms (fetch from source)
- Invalidation: 2-5 ms (broadcast)

## ğŸ” Security Considerations

### Communication Security

- TLS encryption untuk inter-node communication (optional)
- Certificate-based authentication
- Message signing untuk integrity

### Access Control

- RBAC untuk API endpoints
- Token-based authentication
- Audit logging semua operations

## ğŸ¯ Design Decisions

### Why Raft over Paxos?

- Lebih mudah dipahami dan implement
- Clear leader election
- Good performance

### Why HTTP/JSON over gRPC?

- Simplicity untuk demo
- Easy debugging
- Human-readable
- Can upgrade to gRPC easily

### Why MESI over other protocols?

- Industry standard
- Good balance performance/complexity
- Well-documented

### Why Redis for persistence?

- Fast in-memory operations
- Persistence options (AOF/RDB)
- Simple data structures
- Wide adoption

## ğŸ”„ Future Improvements

1. **Multi-Raft**: Separate Raft groups untuk different services
2. **Read Replicas**: Scale read operations
3. **Sharding**: Horizontal partitioning
4. **Compression**: Reduce network traffic
5. **Encryption**: End-to-end security
6. **Monitoring**: Advanced metrics dan alerting
7. **Auto-scaling**: Dynamic cluster size

---

**Referensi**:

- Raft Paper: https://raft.github.io/raft.pdf
- MESI Protocol: Computer Architecture textbooks
- Distributed Systems: Tanenbaum & Van Steen
